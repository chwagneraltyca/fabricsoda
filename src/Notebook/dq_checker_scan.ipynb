{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fabric Python Notebook - DQ Checker Scan Executor\n",
        "# ============================================================================\n",
        "# Execute data quality checks against Fabric Data Warehouse using Soda Core.\n",
        "#\n",
        "# Architecture: READ (SQL DB) -> YAML -> EXECUTE (Soda) -> PARSE -> WRITE (SQL DB + OneLake)\n",
        "#\n",
        "# Requirements:\n",
        "#   - Fabric Python Notebook (NOT PySpark)\n",
        "#   - soda-core-sqlserver OR soda-core-fabric package\n",
        "#   - pyodbc with ODBC Driver 18\n",
        "#\n",
        "# Authentication Methods (tested via smoke test):\n",
        "#   - type: sqlserver + ActiveDirectoryServicePrincipal (RECOMMENDED)\n",
        "#   - type: fabric + activedirectoryserviceprincipal\n",
        "#   - type: fabric + fabricspark (managed identity)\n",
        "#   - type: sqlserver + trusted_connection\n",
        "# ============================================================================"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DQ Checker - Soda Core Scan Executor\n",
        "\n",
        "**Purpose:** Execute data quality checks against Fabric Data Warehouse using Soda Core.\n",
        "\n",
        "**Architecture:**\n",
        "```\n",
        "READ (SQL DB) -> YAML -> EXECUTE (Soda) -> PARSE -> WRITE (SQL DB + OneLake)\n",
        "```\n",
        "\n",
        "| Step | Function | Source |\n",
        "|------|----------|--------|\n",
        "| READ | Fetch check metadata | SQL DB via pyodbc |\n",
        "| YAML | Generate SodaCL | Proven logic from Legacy |\n",
        "| EXECUTE | Run Soda scan | soda-core-fabric |\n",
        "| PARSE | Extract results | Proven logic from Legacy |\n",
        "| WRITE | Store results | SQL DB + OneLake |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Dependencies\n",
        "\n",
        "Run once per session to install required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Soda packages (run once per session)\n",
        "# - soda-core-sqlserver: Best for Service Principal auth\n",
        "# - soda-core-fabric: For fabricspark auth (managed identity)\n",
        "%pip install soda-core-sqlserver soda-core-fabric pyodbc --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n",
        "\n",
        "Set execution parameters and connection details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CONFIGURATION - Edit these values or pass via pipeline parameters\n",
        "# ============================================================================\n",
        "\n",
        "# =============================================================================\n",
        "# EXECUTION MODE\n",
        "# =============================================================================\n",
        "# Set SMOKE_TEST = True to test Soda connection without metadata DB\n",
        "# Set SMOKE_TEST = False for full suite execution\n",
        "SMOKE_TEST = True\n",
        "\n",
        "# Execution parameter (passed from Fabric Pipeline or set manually)\n",
        "SUITE_ID = 1\n",
        "\n",
        "# =============================================================================\n",
        "# METADATA DATABASE (Fabric SQL Database)\n",
        "# =============================================================================\n",
        "META_DB_SERVER = \"yndfhalt62tejhuwlqaqhskcgu-n3hvjhr6avluxog2ch3jdnb5ya.database.fabric.microsoft.com\"\n",
        "META_DB_NAME = \"soda_db-3dbb8254-b235-48a7-b66b-6b321f471b52\"  # Full artifact name for pyodbc\n",
        "\n",
        "# =============================================================================\n",
        "# TARGET DATA WAREHOUSE (Fabric DWH - where data lives)\n",
        "# =============================================================================\n",
        "DWH_SERVER = \"yndfhalt62tejhuwlqaqhskcgu-n3hvjhr6avluxog2ch3jdnb5ya.datawarehouse.fabric.microsoft.com\"\n",
        "DWH_DATABASE = \"sample_dwh\"\n",
        "\n",
        "# =============================================================================\n",
        "# KEY VAULT CONFIGURATION\n",
        "# Secrets are stored securely in Azure Key Vault\n",
        "# =============================================================================\n",
        "KEY_VAULT_URI = \"https://chwakv.vault.azure.net/\"\n",
        "SECRET_NAME = \"dq-checker-spn-secret\"\n",
        "\n",
        "# =============================================================================\n",
        "# SERVICE PRINCIPAL CREDENTIALS\n",
        "# Client ID is not a secret, Client Secret comes from Key Vault\n",
        "# =============================================================================\n",
        "CLIENT_ID = \"b9450ac1-a673-4e67-87de-1b3b94036a40\"\n",
        "CLIENT_SECRET = None  # Will be loaded from Key Vault at runtime\n",
        "\n",
        "# =============================================================================\n",
        "# ONELAKE PATHS\n",
        "# =============================================================================\n",
        "LAKEHOUSE_PATH = \"/lakehouse/default/Files\"\n",
        "LOGS_FOLDER = \"dq_logs\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Imports & Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard library\n",
        "import json\n",
        "import re\n",
        "import uuid\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Optional, Any\n",
        "\n",
        "# Data handling\n",
        "import pandas as pd\n",
        "\n",
        "# Database\n",
        "import pyodbc\n",
        "\n",
        "# Soda Core\n",
        "from soda.scan import Scan\n",
        "\n",
        "# Fabric utilities (always available in Fabric Python notebooks)\n",
        "import notebookutils\n",
        "\n",
        "# Generate unique run ID\n",
        "RUN_ID = str(uuid.uuid4())[:8]\n",
        "print(f\"Run ID: {RUN_ID}\")\n",
        "print(f\"Suite ID: {SUITE_ID}\")\n",
        "\n",
        "# Load secret from Key Vault\n",
        "print(f\"\\nLoading secret from Key Vault: {KEY_VAULT_URI}\")\n",
        "CLIENT_SECRET = notebookutils.credentials.getSecret(KEY_VAULT_URI, SECRET_NAME)\n",
        "print(\"Secret loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Database Connection\n",
        "\n",
        "Connect to metadata SQL DB using pyodbc + Service Principal.\n",
        "This approach requires SP to have db_datareader, db_datawriter, and EXECUTE permissions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_metadata_connection():\n",
        "    \"\"\"\n",
        "    Connect to metadata SQL DB using pyodbc + Service Principal.\n",
        "\n",
        "    This approach uses direct database connection with Service Principal auth.\n",
        "    Requires SP to have database-level permissions (db_datareader, db_datawriter, EXECUTE).\n",
        "    Credentials are retrieved from Azure Key Vault via notebookutils.\n",
        "\n",
        "    Returns:\n",
        "        pyodbc.Connection: Database connection\n",
        "    \"\"\"\n",
        "    conn_str = (\n",
        "        f\"Driver={{ODBC Driver 18 for SQL Server}};\"\n",
        "        f\"Server={META_DB_SERVER},1433;\"\n",
        "        f\"Database={META_DB_NAME};\"\n",
        "        f\"Authentication=ActiveDirectoryServicePrincipal;\"\n",
        "        f\"UID={CLIENT_ID};\"\n",
        "        f\"PWD={CLIENT_SECRET};\"\n",
        "        f\"Encrypt=yes;\"\n",
        "        f\"TrustServerCertificate=no;\"\n",
        "    )\n",
        "    return pyodbc.connect(conn_str)\n",
        "\n",
        "\n",
        "def get_dwh_config_yaml(auth_method: str = \"sqlserver_spn\") -> str:\n",
        "    \"\"\"\n",
        "    Generate Soda connection YAML for Fabric DWH.\n",
        "\n",
        "    Args:\n",
        "        auth_method: Authentication method to use\n",
        "            - \"sqlserver_spn\": soda-core-sqlserver with Service Principal (RECOMMENDED)\n",
        "            - \"fabric_spn\": soda-core-fabric with Service Principal\n",
        "            - \"fabric_spark\": soda-core-fabric with managed identity (fabricspark)\n",
        "            - \"sqlserver_trusted\": soda-core-sqlserver with trusted_connection\n",
        "\n",
        "    Returns:\n",
        "        Soda configuration YAML string\n",
        "    \"\"\"\n",
        "    if auth_method == \"sqlserver_spn\":\n",
        "        # RECOMMENDED: soda-core-sqlserver with Service Principal\n",
        "        # This is the most reliable method based on smoke testing\n",
        "        return f\"\"\"\n",
        "data_source fabric_dwh:\n",
        "  type: sqlserver\n",
        "  driver: ODBC Driver 18 for SQL Server\n",
        "  host: {DWH_SERVER}\n",
        "  port: '1433'\n",
        "  database: {DWH_DATABASE}\n",
        "  authentication: ActiveDirectoryServicePrincipal\n",
        "  username: {CLIENT_ID}\n",
        "  password: {CLIENT_SECRET}\n",
        "  encrypt: true\n",
        "  trust_server_certificate: false\n",
        "\"\"\"\n",
        "\n",
        "    elif auth_method == \"fabric_spn\":\n",
        "        # soda-core-fabric with Service Principal\n",
        "        return f\"\"\"\n",
        "data_source fabric_dwh:\n",
        "  type: fabric\n",
        "  driver: ODBC Driver 18 for SQL Server\n",
        "  host: {DWH_SERVER}\n",
        "  database: {DWH_DATABASE}\n",
        "  authentication: activedirectoryserviceprincipal\n",
        "  client_id: {CLIENT_ID}\n",
        "  client_secret: {CLIENT_SECRET}\n",
        "  encrypt: true\n",
        "\"\"\"\n",
        "\n",
        "    elif auth_method == \"fabric_spark\":\n",
        "        # soda-core-fabric with managed identity (fabricspark)\n",
        "        return f\"\"\"\n",
        "data_source fabric_dwh:\n",
        "  type: fabric\n",
        "  driver: ODBC Driver 18 for SQL Server\n",
        "  host: {DWH_SERVER}\n",
        "  database: {DWH_DATABASE}\n",
        "  authentication: fabricspark\n",
        "  encrypt: true\n",
        "\"\"\"\n",
        "\n",
        "    elif auth_method == \"sqlserver_trusted\":\n",
        "        # soda-core-sqlserver with trusted connection\n",
        "        return f\"\"\"\n",
        "data_source fabric_dwh:\n",
        "  type: sqlserver\n",
        "  driver: ODBC Driver 18 for SQL Server\n",
        "  host: {DWH_SERVER}\n",
        "  port: '1433'\n",
        "  database: {DWH_DATABASE}\n",
        "  trusted_connection: true\n",
        "  encrypt: true\n",
        "\"\"\"\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown auth_method: {auth_method}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Component: READ\n",
        "\n",
        "Fetch check definitions from metadata database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def read_suite_checks(conn, suite_id: int) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Fetch enabled checks for suite execution.\n",
        "\n",
        "    Args:\n",
        "        conn: pyodbc.Connection\n",
        "        suite_id: Suite to execute\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with check definitions\n",
        "    \"\"\"\n",
        "    query = f\"\"\"\n",
        "        SELECT c.*, tc.schema_name\n",
        "        FROM vw_checks_complete c\n",
        "        JOIN suites_testcases st ON c.testcase_id = st.testcase_id\n",
        "        WHERE st.suite_id = {suite_id} AND c.is_enabled = 1\n",
        "        ORDER BY c.check_id\n",
        "    \"\"\"\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute(query)\n",
        "    columns = [column[0] for column in cursor.description]\n",
        "    rows = cursor.fetchall()\n",
        "    return pd.DataFrame.from_records(rows, columns=columns)\n",
        "\n",
        "\n",
        "def create_execution_log(conn, run_id: str, suite_id: int) -> int:\n",
        "    \"\"\"\n",
        "    Create execution log entry with status='running'.\n",
        "\n",
        "    Args:\n",
        "        conn: pyodbc.Connection\n",
        "        run_id: Unique run identifier\n",
        "        suite_id: Suite to execute\n",
        "\n",
        "    Returns:\n",
        "        execution_log_id\n",
        "    \"\"\"\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute(f\"EXEC sp_create_execution_log @run_id='{run_id}', @suite_id={suite_id}\")\n",
        "    execution_log_id = int(cursor.fetchone()[0])\n",
        "    conn.commit()\n",
        "    return execution_log_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Component: YAML Generator\n",
        "\n",
        "Generate SodaCL YAML from check definitions.\n",
        "\n",
        "**Status:** PROVEN - Copied from Legacy `yaml_generator.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YAML Helper Functions (from yaml_helpers.py)\n",
        "\n",
        "YAML_SPECIAL_CHARS = {':', '#', '{', '}', '[', ']', '&', '*', '!', '|', '>', '@', '`', '%'}\n",
        "\n",
        "\n",
        "def yaml_safe_value(value: Optional[str]) -> str:\n",
        "    \"\"\"Make a string value safe for YAML output.\"\"\"\n",
        "    if value is None:\n",
        "        return ''\n",
        "    if not isinstance(value, str):\n",
        "        value = str(value)\n",
        "    value = value.strip()\n",
        "    if not value:\n",
        "        return ''\n",
        "\n",
        "    # Multi-line requires block scalar\n",
        "    if '\\n' in value:\n",
        "        indented = '\\n        '.join(value.split('\\n'))\n",
        "        return f'|\\n        {indented}'\n",
        "\n",
        "    # Check if quoting is needed\n",
        "    needs_quoting = (\n",
        "        any(c in value for c in YAML_SPECIAL_CHARS) or\n",
        "        value.startswith(\"'\") or value.startswith('\"') or\n",
        "        value.startswith(' ') or value.endswith(' ')\n",
        "    )\n",
        "\n",
        "    if needs_quoting:\n",
        "        escaped = value.replace('\\\\', '\\\\\\\\').replace('\"', '\\\\\"')\n",
        "        return f'\"{escaped}\"'\n",
        "\n",
        "    return value\n",
        "\n",
        "\n",
        "def yaml_safe_filter(filter_condition: Optional[str]) -> str:\n",
        "    \"\"\"Make a filter condition safe for YAML output.\"\"\"\n",
        "    if filter_condition is None:\n",
        "        return ''\n",
        "    if not isinstance(filter_condition, str):\n",
        "        filter_condition = str(filter_condition)\n",
        "    filter_condition = filter_condition.strip()\n",
        "    if not filter_condition:\n",
        "        return ''\n",
        "    return yaml_safe_value(filter_condition)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YAML Generator (from yaml_generator.py)\n",
        "\n",
        "SUPPORTED_METRICS = [\n",
        "    # Core metrics (14)\n",
        "    'row_count', 'missing_count', 'missing_percent',\n",
        "    'duplicate_count', 'duplicate_percent',\n",
        "    'min', 'max', 'avg', 'sum',\n",
        "    'invalid_count', 'invalid_percent', 'valid_count',\n",
        "    'avg_length', 'min_length',\n",
        "    # Advanced metrics (6)\n",
        "    'reference', 'user_defined', 'custom_sql',\n",
        "    'scalar_comparison', 'freshness', 'schema'\n",
        "]\n",
        "\n",
        "\n",
        "def generate_yaml_from_checks(checks_df: pd.DataFrame, schema_in_connection: Optional[str] = None) -> str:\n",
        "    \"\"\"\n",
        "    Generate SodaCL YAML from checks DataFrame.\n",
        "\n",
        "    Args:\n",
        "        checks_df: DataFrame with check definitions\n",
        "        schema_in_connection: Schema injected into connection config\n",
        "\n",
        "    Returns:\n",
        "        SodaCL YAML string\n",
        "    \"\"\"\n",
        "    if checks_df.empty:\n",
        "        return \"# No checks found\\n\"\n",
        "\n",
        "    yaml_lines = []\n",
        "\n",
        "    for (schema_name, table_name), table_checks in checks_df.groupby(['schema_name', 'table_name']):\n",
        "        # Build table identifier\n",
        "        table_str = str(table_name)\n",
        "        has_special_chars = ' ' in table_str or '-' in table_str\n",
        "\n",
        "        if has_special_chars:\n",
        "            fully_qualified_table = f'\"{table_str}\"'\n",
        "        elif schema_in_connection and pd.notna(schema_name) and str(schema_name) == schema_in_connection:\n",
        "            fully_qualified_table = table_str\n",
        "        elif pd.notna(schema_name) and '.' not in table_str:\n",
        "            fully_qualified_table = f\"{schema_name}.{table_str}\"\n",
        "        else:\n",
        "            fully_qualified_table = table_str\n",
        "\n",
        "        # Collect check YAML\n",
        "        table_check_lines = []\n",
        "        for _, check in table_checks.iterrows():\n",
        "            check_yaml = _generate_check_yaml(check)\n",
        "            table_check_lines.extend(check_yaml)\n",
        "\n",
        "        if table_check_lines:\n",
        "            yaml_lines.append(f\"checks for {fully_qualified_table}:\")\n",
        "            yaml_lines.extend(table_check_lines)\n",
        "            yaml_lines.append(\"\")\n",
        "\n",
        "    return \"\\n\".join(yaml_lines)\n",
        "\n",
        "\n",
        "def _generate_check_yaml(check: pd.Series) -> List[str]:\n",
        "    \"\"\"Generate YAML lines for a single check.\"\"\"\n",
        "    lines = []\n",
        "    metric = check['metric']\n",
        "    column = check.get('column_name_quoted') or check.get('column_name')\n",
        "\n",
        "    # Metrics that require a column\n",
        "    column_required_metrics = [\n",
        "        'missing_count', 'missing_percent',\n",
        "        'duplicate_count', 'duplicate_percent',\n",
        "        'min', 'max', 'avg', 'sum',\n",
        "        'invalid_count', 'invalid_percent', 'valid_count',\n",
        "        'avg_length', 'min_length', 'reference'\n",
        "    ]\n",
        "\n",
        "    # Special handling for advanced metrics\n",
        "    if metric == 'freshness':\n",
        "        return _generate_freshness_yaml(check)\n",
        "    if metric == 'schema':\n",
        "        return _generate_schema_yaml(check)\n",
        "    if metric == 'reference':\n",
        "        return _generate_reference_yaml(check)\n",
        "    if metric in ['user_defined', 'custom_sql']:\n",
        "        return _generate_custom_sql_yaml(check)\n",
        "    if metric == 'scalar_comparison':\n",
        "        return _generate_scalar_comparison_yaml(check)\n",
        "\n",
        "    # Standard metrics\n",
        "    if pd.notna(column) and metric in column_required_metrics:\n",
        "        lines.append(f\"  - {metric}({column}):\")\n",
        "    else:\n",
        "        lines.append(f\"  - {metric}:\")\n",
        "\n",
        "    # Check name with ID for linking\n",
        "    check_name = check['check_name']\n",
        "    if pd.notna(check.get('check_id')):\n",
        "        check_name = f\"{check_name} [check_id:{check['check_id']}]\"\n",
        "    lines.append(f'      name: \"{check_name}\"')\n",
        "\n",
        "    # Add thresholds\n",
        "    lines.extend(_generate_threshold_yaml(check))\n",
        "\n",
        "    return lines\n",
        "\n",
        "\n",
        "def _generate_threshold_yaml(check: pd.Series) -> List[str]:\n",
        "    \"\"\"Generate warn/fail threshold YAML.\"\"\"\n",
        "    lines = []\n",
        "\n",
        "    # Warn threshold\n",
        "    if pd.notna(check.get('warn_threshold')) and pd.notna(check.get('warn_comparison')):\n",
        "        warn_op = '=' if check['warn_comparison'] == '==' else check['warn_comparison']\n",
        "        lines.append(f\"      warn: when {warn_op} {check['warn_threshold']}\")\n",
        "\n",
        "    # Fail threshold\n",
        "    if pd.notna(check.get('fail_threshold')) and pd.notna(check.get('fail_comparison')):\n",
        "        fail_op = '=' if check['fail_comparison'] == '==' else check['fail_comparison']\n",
        "        lines.append(f\"      fail: when {fail_op} {check['fail_threshold']}\")\n",
        "\n",
        "    return lines\n",
        "\n",
        "\n",
        "def _generate_freshness_yaml(check: pd.Series) -> List[str]:\n",
        "    \"\"\"Generate YAML for freshness check.\"\"\"\n",
        "    lines = []\n",
        "\n",
        "    if (pd.notna(check.get('freshness_column')) and\n",
        "        pd.notna(check.get('freshness_threshold_value')) and\n",
        "        pd.notna(check.get('freshness_threshold_unit'))):\n",
        "\n",
        "        column = check['freshness_column']\n",
        "        threshold_val = check['freshness_threshold_value']\n",
        "        threshold_unit = check['freshness_threshold_unit']\n",
        "\n",
        "        # Remove unnecessary decimals\n",
        "        if isinstance(threshold_val, float) and threshold_val == int(threshold_val):\n",
        "            threshold_val = int(threshold_val)\n",
        "\n",
        "        lines.append(f\"  - freshness({column}) < {threshold_val}{threshold_unit}:\")\n",
        "\n",
        "        check_name = check['check_name']\n",
        "        if pd.notna(check.get('check_id')):\n",
        "            check_name = f\"{check_name} [check_id:{check['check_id']}]\"\n",
        "        lines.append(f'      name: \"{check_name}\"')\n",
        "\n",
        "    return lines\n",
        "\n",
        "\n",
        "def _generate_schema_yaml(check: pd.Series) -> List[str]:\n",
        "    \"\"\"Generate YAML for schema check.\"\"\"\n",
        "    lines = []\n",
        "    lines.append(f\"  - schema:\")\n",
        "\n",
        "    check_name = check['check_name']\n",
        "    if pd.notna(check.get('check_id')):\n",
        "        check_name = f\"{check_name} [check_id:{check['check_id']}]\"\n",
        "    lines.append(f'      name: \"{check_name}\"')\n",
        "\n",
        "    # Required columns\n",
        "    if pd.notna(check.get('schema_required_columns')):\n",
        "        try:\n",
        "            required = json.loads(check['schema_required_columns'])\n",
        "            if required:\n",
        "                lines.append(\"      fail:\")\n",
        "                lines.append(\"        when required column missing:\")\n",
        "                for col in required:\n",
        "                    lines.append(f\"          - {col}\")\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # Forbidden columns\n",
        "    if pd.notna(check.get('schema_forbidden_columns')):\n",
        "        try:\n",
        "            forbidden = json.loads(check['schema_forbidden_columns'])\n",
        "            if forbidden:\n",
        "                if \"      fail:\" not in lines:\n",
        "                    lines.append(\"      fail:\")\n",
        "                lines.append(\"        when forbidden column present:\")\n",
        "                for col in forbidden:\n",
        "                    lines.append(f\"          - {col}\")\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    return lines\n",
        "\n",
        "\n",
        "def _generate_reference_yaml(check: pd.Series) -> List[str]:\n",
        "    \"\"\"Generate YAML for reference (FK) check.\"\"\"\n",
        "    lines = []\n",
        "\n",
        "    if pd.notna(check.get('reference_table')) and pd.notna(check.get('reference_column')):\n",
        "        source_column = check.get('column_name_quoted') or check.get('column_name')\n",
        "        ref_table = check['reference_table']\n",
        "        ref_column = check.get('reference_column_quoted') or check['reference_column']\n",
        "        source_table = check['table_name']\n",
        "        schema_name = check.get('schema_name', 'dbo')\n",
        "\n",
        "        source_fqn = f\"{schema_name}.{source_table}\"\n",
        "        ref_fqn = f\"dbo.{ref_table}\"\n",
        "\n",
        "        lines.append(f\"  - failed rows:\")\n",
        "        check_name = check['check_name']\n",
        "        if pd.notna(check.get('check_id')):\n",
        "            check_name = f\"{check_name} [check_id:{check['check_id']}]\"\n",
        "        lines.append(f'      name: \"{check_name}\"')\n",
        "        lines.append(f\"      fail query: |\")\n",
        "        lines.append(f\"        SELECT * FROM {source_fqn}\")\n",
        "        lines.append(f\"        WHERE {source_column} IS NOT NULL\")\n",
        "        lines.append(f\"          AND {source_column} NOT IN (\")\n",
        "        lines.append(f\"            SELECT {ref_column} FROM {ref_fqn}\")\n",
        "        lines.append(f\"          )\")\n",
        "\n",
        "    return lines\n",
        "\n",
        "\n",
        "def _generate_custom_sql_yaml(check: pd.Series) -> List[str]:\n",
        "    \"\"\"Generate YAML for custom SQL check.\"\"\"\n",
        "    lines = []\n",
        "\n",
        "    if pd.notna(check.get('custom_sql_query')):\n",
        "        custom_sql = str(check['custom_sql_query']).strip()\n",
        "\n",
        "        # Create safe metric name\n",
        "        safe_metric_name = re.sub(r'[^a-zA-Z0-9_]', '_', check['check_name'].lower())\n",
        "        safe_metric_name = re.sub(r'_+', '_', safe_metric_name).strip('_')\n",
        "\n",
        "        # Build metric comparison\n",
        "        fail_comparison = check.get('fail_comparison')\n",
        "        if pd.notna(fail_comparison) and pd.notna(check.get('fail_threshold')):\n",
        "            comparison = '=' if fail_comparison == '==' else fail_comparison\n",
        "            lines.append(f\"  - {safe_metric_name} {comparison} {check['fail_threshold']}:\")\n",
        "        else:\n",
        "            lines.append(f\"  - {safe_metric_name} = 0:\")\n",
        "\n",
        "        check_name = check['check_name']\n",
        "        if pd.notna(check.get('check_id')):\n",
        "            check_name = f\"{check_name} [check_id:{check['check_id']}]\"\n",
        "        lines.append(f'      name: \"{check_name}\"')\n",
        "\n",
        "        lines.append(f\"      {safe_metric_name} query: |\")\n",
        "        for sql_line in custom_sql.split('\\n'):\n",
        "            lines.append(f\"        {sql_line}\")\n",
        "\n",
        "    return lines\n",
        "\n",
        "\n",
        "def _generate_scalar_comparison_yaml(check: pd.Series) -> List[str]:\n",
        "    \"\"\"Generate YAML for scalar comparison check.\"\"\"\n",
        "    lines = []\n",
        "\n",
        "    if pd.notna(check.get('scalar_query_a')) and pd.notna(check.get('scalar_query_b')):\n",
        "        query_a = str(check['scalar_query_a']).strip()\n",
        "        query_b = str(check['scalar_query_b']).strip()\n",
        "        operator = check.get('scalar_operator', '==')\n",
        "\n",
        "        check_name = check['check_name']\n",
        "        if pd.notna(check.get('check_id')):\n",
        "            check_name = f\"{check_name} [check_id:{check['check_id']}]\"\n",
        "\n",
        "        lines.append(f\"  - failed rows:\")\n",
        "        lines.append(f'      name: \"{check_name}\"')\n",
        "        lines.append(f\"      fail query: |\")\n",
        "        lines.append(f\"        WITH comparison AS (\")\n",
        "        lines.append(f\"          SELECT\")\n",
        "        lines.append(f\"            ({query_a}) AS query_a,\")\n",
        "        lines.append(f\"            ({query_b}) AS query_b\")\n",
        "        lines.append(f\"        )\")\n",
        "        lines.append(f\"        SELECT query_a, query_b, query_a - query_b AS difference\")\n",
        "        lines.append(f\"        FROM comparison\")\n",
        "\n",
        "        # Inverted operator logic\n",
        "        where_map = {\n",
        "            '==': 'query_a != query_b',\n",
        "            '!=': 'query_a = query_b',\n",
        "            '>': 'query_a <= query_b',\n",
        "            '>=': 'query_a < query_b',\n",
        "            '<': 'query_a >= query_b',\n",
        "            '<=': 'query_a > query_b'\n",
        "        }\n",
        "        lines.append(f\"        WHERE {where_map.get(operator, 'query_a != query_b')}\")\n",
        "\n",
        "    return lines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Component: EXECUTE\n",
        "\n",
        "Run Soda scan against Fabric DWH."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def execute_soda_scan(yaml_content: str, dwh_config: str, run_id: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Execute Soda scan and return results.\n",
        "\n",
        "    Args:\n",
        "        yaml_content: SodaCL YAML\n",
        "        dwh_config: Soda connection YAML\n",
        "        run_id: Unique run identifier\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with results, logs, and error status\n",
        "    \"\"\"\n",
        "    scan = Scan()\n",
        "    scan.set_data_source_name(\"fabric_dwh\")\n",
        "    scan.set_scan_definition_name(f\"dq_checker_scan_{run_id}\")\n",
        "    scan.add_configuration_yaml_str(dwh_config)\n",
        "    scan.add_sodacl_yaml_str(yaml_content)\n",
        "\n",
        "    scan.execute()\n",
        "\n",
        "    return {\n",
        "        \"results\": scan.get_scan_results(),\n",
        "        \"logs\": scan.get_logs_text(),\n",
        "        \"has_errors\": scan.has_error_logs(),\n",
        "        \"error_logs\": scan.get_error_logs_text() if scan.has_error_logs() else None\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Component: PARSE\n",
        "\n",
        "Extract check results from Soda scan output.\n",
        "\n",
        "**Status:** PROVEN - Copied from Legacy `scan_orchestrator.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_scan_results(scan_results: Dict) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Parse Soda scan results into result records.\n",
        "\n",
        "    Args:\n",
        "        scan_results: Raw Soda scan results\n",
        "\n",
        "    Returns:\n",
        "        List of result dictionaries\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    for check in scan_results.get('checks', []):\n",
        "        # Extract check_id from name (format: \"Check Name [check_id:123]\")\n",
        "        check_name = check.get('name', '')\n",
        "        check_id_match = re.search(r'\\[check_id:(\\d+)\\]', check_name)\n",
        "        check_id = int(check_id_match.group(1)) if check_id_match else None\n",
        "\n",
        "        # Get diagnostics for values and thresholds\n",
        "        diagnostics = check.get('diagnostics', {})\n",
        "        check_value = diagnostics.get('value', check.get('value'))\n",
        "\n",
        "        results.append({\n",
        "            'check_id': check_id,\n",
        "            'check_name': check_name,\n",
        "            'outcome': check.get('outcome', 'unknown'),\n",
        "            'value': check_value\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def count_outcomes(results: List[Dict]) -> Dict[str, int]:\n",
        "    \"\"\"Count pass/fail/warn outcomes.\"\"\"\n",
        "    return {\n",
        "        'total': len(results),\n",
        "        'passed': len([r for r in results if r['outcome'] == 'pass']),\n",
        "        'failed': len([r for r in results if r['outcome'] == 'fail']),\n",
        "        'warned': len([r for r in results if r['outcome'] == 'warn']),\n",
        "        'errors': len([r for r in results if r['outcome'] == 'error'])\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Component: WRITE\n",
        "\n",
        "Store results to SQL DB (via SPs) and OneLake."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def write_results_to_db(conn, execution_log_id: int, run_id: str, results: List[Dict]):\n",
        "    \"\"\"Write individual check results to SQL DB via SP.\"\"\"\n",
        "    cursor = conn.cursor()\n",
        "    for r in results:\n",
        "        # Escape single quotes in strings\n",
        "        check_name = str(r['check_name']).replace(\"'\", \"''\") if r['check_name'] else ''\n",
        "        outcome = str(r['outcome']).replace(\"'\", \"''\") if r['outcome'] else ''\n",
        "        check_id = r['check_id'] if r['check_id'] else 'NULL'\n",
        "        check_value = r['value'] if r['value'] is not None else 'NULL'\n",
        "\n",
        "        query = f\"\"\"EXEC sp_insert_result\n",
        "            @run_id='{run_id}',\n",
        "            @execution_log_id={execution_log_id},\n",
        "            @check_id={check_id},\n",
        "            @check_name='{check_name}',\n",
        "            @check_outcome='{outcome}',\n",
        "            @check_value={check_value}\"\"\"\n",
        "        cursor.execute(query)\n",
        "        cursor.fetchone()  # Consume SP result\n",
        "    conn.commit()\n",
        "\n",
        "\n",
        "def update_execution_log(conn, execution_log_id: int, counts: Dict, yaml_content: str,\n",
        "                         error_message: Optional[str] = None):\n",
        "    \"\"\"Update execution log with completion status via SP.\"\"\"\n",
        "    cursor = conn.cursor()\n",
        "    status = 'failed' if error_message else 'completed'\n",
        "    has_failures = 1 if counts['failed'] > 0 else 0\n",
        "\n",
        "    # Escape single quotes\n",
        "    yaml_escaped = yaml_content.replace(\"'\", \"''\") if yaml_content else ''\n",
        "    error_escaped = error_message.replace(\"'\", \"''\") if error_message else ''\n",
        "    error_param = f\"'{error_escaped}'\" if error_message else 'NULL'\n",
        "\n",
        "    query = f\"\"\"EXEC sp_update_execution_log\n",
        "        @execution_log_id={execution_log_id},\n",
        "        @status='{status}',\n",
        "        @total_checks={counts['total']},\n",
        "        @checks_passed={counts['passed']},\n",
        "        @checks_failed={counts['failed']},\n",
        "        @checks_warned={counts['warned']},\n",
        "        @has_failures={has_failures},\n",
        "        @generated_yaml='{yaml_escaped}',\n",
        "        @error_message={error_param}\"\"\"\n",
        "    cursor.execute(query)\n",
        "    conn.commit()\n",
        "\n",
        "\n",
        "def write_to_onelake(run_id: str, execution_log_id: int, suite_id: int,\n",
        "                     scan_results: Dict, soda_logs: str, yaml_content: str,\n",
        "                     counts: Dict) -> str:\n",
        "    \"\"\"\n",
        "    Write full results JSON to OneLake with Hive-style partitioning.\n",
        "\n",
        "    Folder structure for partition elimination:\n",
        "        dq_logs/year=YYYY/month=MM/day=DD/execution_{run_id}.json\n",
        "\n",
        "    This allows efficient Spark queries like:\n",
        "        SELECT * FROM dq_results WHERE year = 2026 AND month = 1\n",
        "    \"\"\"\n",
        "    now = datetime.utcnow()\n",
        "\n",
        "    full_results = {\n",
        "        \"run_id\": run_id,\n",
        "        \"execution_log_id\": execution_log_id,\n",
        "        \"suite_id\": suite_id,\n",
        "        \"timestamp\": now.isoformat(),\n",
        "        # Partition keys (duplicated for easy access in JSON)\n",
        "        \"year\": now.year,\n",
        "        \"month\": now.month,\n",
        "        \"day\": now.day,\n",
        "        \"summary\": counts,\n",
        "        \"scan_results\": scan_results,\n",
        "        \"soda_logs\": soda_logs,\n",
        "        \"yaml_content\": yaml_content\n",
        "    }\n",
        "\n",
        "    json_content = json.dumps(full_results, indent=2, default=str)\n",
        "    file_name = f\"execution_{run_id}.json\"\n",
        "\n",
        "    # Hive-style partition path for Spark partition elimination\n",
        "    partition_path = f\"year={now.year}/month={now.month:02d}/day={now.day:02d}\"\n",
        "    log_path = f\"{LAKEHOUSE_PATH}/{LOGS_FOLDER}/{partition_path}/{file_name}\"\n",
        "\n",
        "    # Write to OneLake using Fabric notebookutils\n",
        "    notebookutils.fs.put(log_path, json_content, overwrite=True)\n",
        "\n",
        "    return log_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Smoke Test Mode\n",
        "\n",
        "Test Soda connection with a fake YAML (no metadata DB required).\n",
        "Set `SMOKE_TEST = True` in configuration to run this instead of full suite."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_smoke_test() -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Run smoke test to verify Soda connection to DWH.\n",
        "\n",
        "    Tests multiple authentication methods and reports which ones work.\n",
        "    Uses a simple check against INFORMATION_SCHEMA.TABLES (always exists).\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with test results for each auth method\n",
        "    \"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"SMOKE TEST - Testing Soda Authentication Methods\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Target DWH: {DWH_SERVER}\")\n",
        "    print(f\"Database: {DWH_DATABASE}\")\n",
        "\n",
        "    # Simple check that should always pass\n",
        "    SMOKE_CHECK_YAML = \"\"\"\n",
        "checks for INFORMATION_SCHEMA.TABLES:\n",
        "  - row_count > 0:\n",
        "      name: \"Smoke test - tables exist\"\n",
        "\"\"\"\n",
        "\n",
        "    # Auth methods to test\n",
        "    auth_methods = [\n",
        "        (\"sqlserver_spn\", \"soda-core-sqlserver + Service Principal (RECOMMENDED)\"),\n",
        "        (\"fabric_spn\", \"soda-core-fabric + Service Principal\"),\n",
        "        (\"fabric_spark\", \"soda-core-fabric + fabricspark (managed identity)\"),\n",
        "        (\"sqlserver_trusted\", \"soda-core-sqlserver + trusted_connection\"),\n",
        "    ]\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for method_key, method_name in auth_methods:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Testing: {method_name}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        try:\n",
        "            config = get_dwh_config_yaml(method_key)\n",
        "\n",
        "            scan = Scan()\n",
        "            scan.set_data_source_name(\"fabric_dwh\")\n",
        "            scan.set_scan_definition_name(f\"smoke_test_{method_key}\")\n",
        "            scan.add_configuration_yaml_str(config)\n",
        "            scan.add_sodacl_yaml_str(SMOKE_CHECK_YAML)\n",
        "\n",
        "            print(\"  Executing scan...\")\n",
        "            scan.execute()\n",
        "\n",
        "            logs = scan.get_logs_text()\n",
        "            print(\"  Logs (first 500 chars):\")\n",
        "            print(logs[:500])\n",
        "\n",
        "            if scan.has_error_logs():\n",
        "                print(f\"\\n  RESULT: FAILED\")\n",
        "                results[method_key] = {\n",
        "                    \"success\": False,\n",
        "                    \"error\": scan.get_error_logs_text()[:500]\n",
        "                }\n",
        "            else:\n",
        "                scan_results = scan.get_scan_results()\n",
        "                checks = scan_results.get('checks', [])\n",
        "                print(f\"\\n  RESULT: SUCCESS! Checks executed: {len(checks)}\")\n",
        "                results[method_key] = {\n",
        "                    \"success\": True,\n",
        "                    \"checks_executed\": len(checks),\n",
        "                    \"results\": scan_results\n",
        "                }\n",
        "        except Exception as e:\n",
        "            print(f\"  EXCEPTION: {str(e)[:500]}\")\n",
        "            results[method_key] = {\n",
        "                \"success\": False,\n",
        "                \"error\": str(e)[:500]\n",
        "            }\n",
        "\n",
        "    # Summary\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"SMOKE TEST SUMMARY\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    working_methods = []\n",
        "    for method_key, method_name in auth_methods:\n",
        "        status = \"PASS\" if results[method_key][\"success\"] else \"FAIL\"\n",
        "        print(f\"  {method_name}: {status}\")\n",
        "        if results[method_key][\"success\"]:\n",
        "            working_methods.append(method_key)\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    if working_methods:\n",
        "        print(f\"\\nWORKING METHODS: {working_methods}\")\n",
        "        print(f\"RECOMMENDED: {working_methods[0]}\")\n",
        "    else:\n",
        "        print(\"\\nNo authentication method worked!\")\n",
        "        print(\"Check credentials and network connectivity.\")\n",
        "\n",
        "    return {\n",
        "        \"mode\": \"smoke_test\",\n",
        "        \"working_methods\": working_methods,\n",
        "        \"recommended\": working_methods[0] if working_methods else None,\n",
        "        \"results\": results\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Main Execution\n",
        "\n",
        "Orchestrate the complete scan flow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def execute_suite(suite_id: int, run_id: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Execute all checks in a suite.\n",
        "\n",
        "    Args:\n",
        "        suite_id: Suite to execute\n",
        "        run_id: Unique run identifier\n",
        "\n",
        "    Returns:\n",
        "        Execution summary\n",
        "    \"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"DQ CHECKER SCAN - Suite: {suite_id}, Run: {run_id}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    conn = None\n",
        "    execution_log_id = None\n",
        "    yaml_content = \"\"\n",
        "    error_message = None\n",
        "\n",
        "    try:\n",
        "        # ===============================================================\n",
        "        # STEP 1: READ\n",
        "        # ===============================================================\n",
        "        print(\"\\n[1/5] READ: Connecting to metadata DB...\")\n",
        "        conn = get_metadata_connection()\n",
        "\n",
        "        print(\"[1/5] READ: Creating execution log...\")\n",
        "        execution_log_id = create_execution_log(conn, run_id, suite_id)\n",
        "        print(f\"       Execution Log ID: {execution_log_id}\")\n",
        "\n",
        "        print(\"[1/5] READ: Fetching checks...\")\n",
        "        checks_df = read_suite_checks(conn, suite_id)\n",
        "        print(f\"       Found {len(checks_df)} enabled checks\")\n",
        "\n",
        "        if checks_df.empty:\n",
        "            print(\"       No checks to execute. Exiting.\")\n",
        "            return {\"run_id\": run_id, \"status\": \"no_checks\", \"total\": 0}\n",
        "\n",
        "        # ===============================================================\n",
        "        # STEP 2: YAML\n",
        "        # ===============================================================\n",
        "        print(\"\\n[2/5] YAML: Generating SodaCL...\")\n",
        "        yaml_content = generate_yaml_from_checks(checks_df)\n",
        "        print(f\"       Generated {len(yaml_content)} bytes of YAML\")\n",
        "\n",
        "        # ===============================================================\n",
        "        # STEP 3: EXECUTE\n",
        "        # ===============================================================\n",
        "        print(\"\\n[3/5] EXECUTE: Running Soda scan...\")\n",
        "        dwh_config = get_dwh_config_yaml()\n",
        "        scan_output = execute_soda_scan(yaml_content, dwh_config, run_id)\n",
        "\n",
        "        scan_results = scan_output['results']\n",
        "        soda_logs = scan_output['logs']\n",
        "\n",
        "        if scan_output['has_errors']:\n",
        "            print(f\"       WARNING: Scan had errors\")\n",
        "            error_message = scan_output['error_logs']\n",
        "\n",
        "        print(f\"       Executed {len(scan_results.get('checks', []))} checks\")\n",
        "\n",
        "        # ===============================================================\n",
        "        # STEP 4: PARSE\n",
        "        # ===============================================================\n",
        "        print(\"\\n[4/5] PARSE: Extracting results...\")\n",
        "        results = parse_scan_results(scan_results)\n",
        "        counts = count_outcomes(results)\n",
        "        print(f\"       Passed: {counts['passed']}, Failed: {counts['failed']}, Warned: {counts['warned']}\")\n",
        "\n",
        "        # ===============================================================\n",
        "        # STEP 5: WRITE\n",
        "        # ===============================================================\n",
        "        print(\"\\n[5/5] WRITE: Storing results...\")\n",
        "\n",
        "        print(\"       Writing to SQL DB (via SPs)...\")\n",
        "        write_results_to_db(conn, execution_log_id, run_id, results)\n",
        "        update_execution_log(conn, execution_log_id, counts, yaml_content, error_message)\n",
        "\n",
        "        print(\"       Writing to OneLake...\")\n",
        "        log_path = write_to_onelake(run_id, execution_log_id, suite_id,\n",
        "                                     scan_results, soda_logs, yaml_content, counts)\n",
        "        print(f\"       JSON log: {log_path}\")\n",
        "\n",
        "        # ===============================================================\n",
        "        # DONE\n",
        "        # ===============================================================\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"SCAN COMPLETE\")\n",
        "        print(f\"  Run ID: {run_id}\")\n",
        "        print(f\"  Total:  {counts['total']}\")\n",
        "        print(f\"  Passed: {counts['passed']}\")\n",
        "        print(f\"  Failed: {counts['failed']}\")\n",
        "        print(f\"  Warned: {counts['warned']}\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        return {\n",
        "            \"run_id\": run_id,\n",
        "            \"execution_log_id\": execution_log_id,\n",
        "            \"status\": \"completed\",\n",
        "            \"has_failures\": counts['failed'] > 0,\n",
        "            **counts\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        error_message = str(e)\n",
        "        print(f\"\\nERROR: {error_message}\")\n",
        "\n",
        "        # Update execution log with error\n",
        "        if conn and execution_log_id:\n",
        "            try:\n",
        "                update_execution_log(conn, execution_log_id,\n",
        "                                    {'total': 0, 'passed': 0, 'failed': 0, 'warned': 0},\n",
        "                                    yaml_content, error_message)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        raise\n",
        "\n",
        "    finally:\n",
        "        # Close pyodbc connection\n",
        "        if conn:\n",
        "            try:\n",
        "                conn.close()\n",
        "            except:\n",
        "                pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Run Scan\n",
        "\n",
        "Execute smoke test or full suite based on configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execute based on mode\n",
        "if SMOKE_TEST:\n",
        "    print(\"Running in SMOKE TEST mode...\")\n",
        "    print(\"Testing Soda authentication methods against target DWH\")\n",
        "    print(\"Set SMOKE_TEST = False for full suite execution\\n\")\n",
        "    result = run_smoke_test()\n",
        "else:\n",
        "    print(\"Running in FULL SUITE mode...\")\n",
        "    result = execute_suite(SUITE_ID, RUN_ID)\n",
        "\n",
        "# Display result\n",
        "print(f\"\\nResult: {json.dumps(result, indent=2)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Pipeline Integration (Optional)\n",
        "\n",
        "Uncomment to fail the pipeline if any checks failed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to fail pipeline on DQ failures\n",
        "# if result.get('has_failures'):\n",
        "#     raise Exception(f\"DQ validation failed: {result['failed']} checks failed out of {result['total']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Appendix A: Generated YAML Preview\n",
        "\n",
        "Uncomment to view the generated SodaCL YAML for debugging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preview generated YAML (for debugging)\n",
        "# print(yaml_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Appendix B: Pre-installed vs Pip Packages\n",
        "\n",
        "**Pre-installed in Fabric Python Notebooks:**\n",
        "- notebookutils, pandas, DuckDB, Polars, Scikit-learn, delta-rs\n",
        "- Matplotlib, Seaborn, Plotly, pyodbc\n",
        "\n",
        "**Installed via pip (line 46):**\n",
        "- soda-core-sqlserver, soda-core-fabric"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "jupyter",
      "jupyter_kernel_name": "python3.11"
    },
    "kernelspec": {
      "name": "jupyter",
      "display_name": "Jupyter"
    },
    "language_info": {
      "name": "python"
    },
    "microsoft": {
      "language": "python",
      "language_group": "jupyter_python"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "dependencies": {}
  },
  "nbformat": 4,
  "nbformat_minor": 5
}